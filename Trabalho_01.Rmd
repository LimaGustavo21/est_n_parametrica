---
title: "Trabalho No.1"
output: html_notebook
---

# Exercício  

## 1. Variável Aleatória Bernoulli  

Seja \( X \sim \text{Bernoulli} \left(\frac{1}{2} \right) \) e considere todas as possíveis amostras aleatórias de tamanho \( n = 4 \). Calcule \( \bar{X}_n \) e \( S_n^2 \) para cada uma das dezesseis amostras.  


```{r}
set.seed(123)

amostras <- expand.grid(rep(list(c(0,1)), 4)); amostras

medias <- apply(amostras,1, mean); medias

variancia <- apply(amostras,1, var); variancia

```
 

##### Encontre a função de probabilidade de \( \bar{X}_n \) e \( S_n^2 \).

```{r}
#Função de Probabilidade para a MÉDIA;
freq_media <- table(medias)
prob_media <- freq_media / sum(freq_media); prob_media

#Função de Probabilidade para a VARIÂNCIA;

freq_var <- table(variancia)
prob_var <- freq_var / sum(freq_var); prob_var


```


### 2. Função Distância \( \Delta(F, G) \)

Considere a função distância \( \Delta(F, G) \):

\[
\Delta(F, G) = \int_{-\infty}^{\infty} \left[ F(x) - G(x) \right]^2 \cdot \frac{1}{2} \left[ F'(x) + G'(x) \right]^2 \, dx,
\]

onde \( F \) e \( G \) são distribuições absolutamente contínuas.

Sejam \( X_1, \dots, X_m \) uma amostra aleatória de \( F \) e \( Y_1, \dots, Y_n \) uma amostra aleatória de \( G \), ambas independentes.

Sabemos que a **U-estatística** \( U(X, Y) \) é dada por:

\[
U(X, Y) = \frac{1}{{m \choose 2} \cdot {n \choose 2}} \sum_{i_1 < i_2} \sum_{k_1 < k_2} \varphi(X_{i_1}, X_{i_2}, Y_{k_1}, Y_{k_2}),
\]

onde

\[
\varphi(X_1, X_2, Y_1, Y_2) =
\begin{cases}
1, & \text{se } \max(X_1, X_2) < \min(Y_1, Y_2) \text{ ou se } \max(Y_1, Y_2) < \min(X_1, X_2), \\
0, & \text{caso contrário.}
\end{cases}
\]

```{r}
phi <- function(X1, X2, Y1, Y2) {
  
  if (max(X1, X2) < min(Y1, Y2) || max(Y1, Y2) < min(X1, X2)) {
    return(1)
  } else {
    return(0)
  }
}
```


Esta fórmula permite encontrar o **estimador não viciado de mínima variância** para \( \Delta(F, G) \) como:

\[
\hat{\Delta}(F, G) = \frac{1}{2} U(X, Y) - \frac{1}{6}.
\]



##### A) Escreva uma função R para encontrar estimativas de ∆( b F, G), considerando valores diversos de tamanhos de cada amostra.

```{r}

delta_est <- function(X, Y) {
  
  X <- sort(X)
  Y <- sort(Y)
  
  m <- length(X)
  n <- length(Y)
  
  u_sum <- 0
  
  for (i1 in 1:(m-1)) {
    for (i2 in (i1+1):m) {
      for (k1 in 1:(n-1)) {
        for (k2 in (k1+1):n) {
          u_sum <- u_sum + phi(X[i1], X[i2], Y[k1], Y[k2])
        }
      }
    }
  }
  
  # Estatística U(X,Y)
  u_stat <- (1 / (choose(m, 2) * choose(n, 2))) * u_sum
  
  # Estimativa de Δ(F, G)
  delta_est <- (1 / 2) * u_stat - 1/6
  return(delta_est)
}

```


```{r}
#Comparando duas normais com médias diferentes

f <-  rnorm(20, 10, 0.5)
g <-  rnorm(35,0,0.5)
  
delta_est(f,g) 


#Comparando duas normais com médias iguais


f <-  rnorm(25, 0, 2)
g <-  rnorm(35,0,2)

delta_est(f,g) 

#Comparando uma Normal com exponência

f <- rnorm(35, 0, 1)
g <- rexp(50,2)

delta_est(f,g) 

```

#####  B) Gere amostras de tamanho 50, 100 e 150 das distribuições (i) Normal padrão, (ii) Cauchy padrão e (iii) t-Student(4). Considere estas como possíveis distribuições de referência.

```{r}
n_50 <- rnorm(50, mean = 0, sd = 1)
c_50 <- rcauchy(50, location = 0, scale = 1)
t_50 <- rt(50, df = 4)

n_100 <- rnorm(100, mean = 0, sd = 1)
c_100 <- rcauchy(100, location = 0, scale = 1)
t_100 <- rt(100, df = 4)

n_150 <- rnorm(150, mean = 0, sd = 1)
c_150 <- rcauchy(150, location = 0, scale = 1)
t_150 <- rt(150, df = 4)



# Conf o layout para 3 gráficos lado a lado
par(mfrow = c(3, 3))

# Distribuição Normal
hist(n_50, main = "Normal (n = 50)")


hist(n_100, main = "Normal (n = 100)")


hist(n_150, main = "Normal (n = 150)")


# Distribuição Cauchy
hist(c_50,  main = "Cauchy (n = 50)")

hist(c_100, main = "Cauchy (n = 100)")

hist(c_150, main = "Cauchy (n = 150)")

# Distribuição t-Student
hist(t_50,main = "t-Student (n = 50)")

hist(t_100, main = "t-Student (n = 100)")

hist(t_150,main = "t-Student (n = 150)")

par(mfrow = c(1, 1))
```


##### (c) Calcule as estimativas de ∆( b F, G) para cada amostra das distribuições de referência F obtidas no item (b) e a amostra da distribuição desconhecida G, obtida no arquivo de dados anexo.




```{r}
amostra_G <- scan("dados.csv", what = numeric(), sep = "\n")
```
 - Distribuição Normal Padrão e Amostra desconhecida G;
 
```{r}
delta_est(n_50,amostra_G)

delta_est(n_100,amostra_G)

delta_est(n_150,amostra_G)
```
- Distribuição T-Student (4) e Amostra desconhecida G

```{r}
delta_est(t_50,amostra_G)

delta_est(t_100,amostra_G)

delta_est(t_150,amostra_G)

```
- Distribuição Cauchy Padrão e Amostra desconhecida G

```{r}
delta_est(c_50,amostra_G)

delta_est(c_100,amostra_G)

delta_est(c_150,amostra_G)
```

##### (d) Segundo seus cálculos, qual é a distribuição desconhecida G?

 - Os resultados da função distância indica uma proximidade muito grande entre a função G e a normal Padrão, o que indica que G segue uma N(0,1).



### 3. Estimador não Viciado de mínima variância

##### Sejam X1,X2,···,Xm e Y1,Y2,···,Yn amostras independentes de duas distribuições absolutamente contínuas. Encontre o estimador não viciado de mínima variância de: 

```{r}
X <- rnorm(10000,5,3)

Y <- rexp(15000,4)
```

 -  (a) E(XY)
```{r}

mean(X)*mean(Y) #Se independentes, a E(XY) = E(X)*E(Y)


```
 -  (b) Var(X + Y).
```{r}

var(X) + var(Y) #Se independentes, VAR(X + Y) = VAR(X) + VAR(Y)

```
### 4. Função densidade Estimada

Podemos considerar a densidade kernel estimada como a função de densidade de uma amostra, como obter então novas amostras?  

Como o estimador kernel da função de densidade  

$$
f_{bn}(x) = \frac{1}{n h_n} \sum_{i=1}^{n} K \left( \frac{x - X_i}{h_n} \right)
$$  

é uma mistura de \(n\) componentes de kernel, cada um pode ser amostrado independentemente.  

A única parte que pode exigir tratamento especial é a amostragem da densidade \(K\), embora para a maioria dos kernels implementados R possam ser encontradas funções de amostragem específicas.  

Consideremos os dados em:  


```{r}
phipsi <- read.csv("http://leg.ufpr.br/~lucambio/Nonparam/phipsi.csv", 
                   sep = ",", header = TRUE)


psi <- phipsi$psi
```

Execute o seguinte algoritmo para gerar amostras de tamanho N, a
partir da densidade kernel estimada dos dados em phipsi:

 - (i)  Escolha i ∈ {1,···,n} aleatoriamente.;
```{r}
n <- length(psi)

hn <- density(psi)$bw

i <- sample(1:n,size = 1)
```
 - (ii) Obtenha um amostra de tamanho 1 da densidade K, caso utilizese a densidade gaussiana, a média é Xi e o desvio padrão é hn;
```{r}

  i <- sample(1:n, 1)  
  amostra_kernel <- rnorm(1, mean = psi[i], sd = hn)  
```
 
 - (iii) Repita os passos anteriores N vezes;
```{r}
amostra_kernel <- numeric(600)

for (j in 1:600) {
  i <- sample(1:n, 1)  # Passo (i)
  amostra_kernel[j] <- rnorm(1, mean = psi[i], sd = hn)  # Passo (ii)
}
```

Obtenha uma amostra de tamanho N = 600 da variável ψ (phipsi$psi)
utilizando o algoritmo acima e compare a densidade estimada da nova
amostra com a densidade estimada de ψ. Esta compara¸c˜ao pode ser
graficamente.
```{r}
hist(amostra_kernel)

hist(psi)
```


